{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f3641cb-a2e0-4118-8d3e-8adebee69d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import matplotlib.pyplot as plt  # Import for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b90fde-54c5-4175-93ff-c34db5531854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for BSDS500\n",
    "class BSDS500Dataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        \n",
    "        # Load image file paths, filtering for valid image extensions\n",
    "        self.image_files = [\n",
    "            os.path.join(root_dir, 'images', split, f)\n",
    "            for f in os.listdir(os.path.join(root_dir, 'images', split))\n",
    "            if f.endswith(('.png', '.jpg', '.jpeg'))  # Ensure valid image extensions\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')  # Convert to RGB\n",
    "        except (UnidentifiedImageError, OSError) as e:\n",
    "            print(f\"Error loading image {img_name}: {e}\")\n",
    "            return None  # Handle the error gracefully\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f765889-5fe7-4a43-85a2-6f96ce596930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for training and validation datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((33, 33))  # Resize for SRCNN input size\n",
    "])\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "root_dir = \"C:/Users/IPS/Downloads/Super-Resolution-image-using-CNN-in-PyTorch/BSR/BSDS500/data\"\n",
    "train_dataset = BSDS500Dataset(root_dir=root_dir, split='train', transform=transform)\n",
    "val_dataset = BSDS500Dataset(root_dir=root_dir, split='val', transform=transform)\n",
    "test_dataset = BSDS500Dataset(root_dir=root_dir, split='test', transform=transform)\n",
    "\n",
    "# Check if datasets are not empty\n",
    "if len(train_dataset) == 0 or len(val_dataset) == 0 or len(test_dataset) == 0:\n",
    "    raise ValueError(\"Training, validation or test dataset is empty. Please check your data paths.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315dfc6-c7f8-4506-8136-8f5421f96382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8051c41b-7981-4929-adbc-03a4a3f534f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41199848-8cf2-4deb-b311-12bd6620b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRCNN Model Definition\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SRCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30c673f7-63f6-437d-a6e2-5a1a9122ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69aab15a-2ab1-4929-9825-3963312911d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate metrics (as defined previously)\n",
    "def calculate_metrics(output, target):\n",
    "    output_np = output.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    target_np = target.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    \n",
    "    ssim_val = np.mean([ssim(o, t, data_range=t.max() - t.min(), channel_axis=-1) for o, t in zip(output_np, target_np)])\n",
    "    psnr_val = np.mean([psnr(t, o, data_range=t.max() - t.min()) for o, t in zip(output_np, target_np)])\n",
    "    \n",
    "    mse_val = np.mean((output_np - target_np) ** 2)\n",
    "    return ssim_val, psnr_val, mse_val\n",
    "\n",
    "# Training loop (as defined previously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de3f1a77-b70f-41db-834d-9c2638ec0326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/64], Loss: 0.0050\n",
      "Epoch [1/10], Step [20/64], Loss: 0.0029\n",
      "Epoch [1/10], Step [30/64], Loss: 0.0024\n",
      "Epoch [1/10], Step [40/64], Loss: 0.0024\n",
      "Epoch [1/10], Step [50/64], Loss: 0.0015\n",
      "Epoch [1/10], Step [60/64], Loss: 0.0013\n",
      "Validation - Epoch [1/10], SSIM: 0.7352, PSNR: 18.1953, MSE: 0.0126\n",
      "Epoch [2/10], Step [10/64], Loss: 0.0008\n",
      "Epoch [2/10], Step [20/64], Loss: 0.0009\n",
      "Epoch [2/10], Step [30/64], Loss: 0.0009\n",
      "Epoch [2/10], Step [40/64], Loss: 0.0004\n",
      "Epoch [2/10], Step [50/64], Loss: 0.0006\n",
      "Epoch [2/10], Step [60/64], Loss: 0.0005\n",
      "Validation - Epoch [2/10], SSIM: 0.8087, PSNR: 20.1826, MSE: 0.0084\n",
      "Epoch [3/10], Step [10/64], Loss: 0.0005\n",
      "Epoch [3/10], Step [20/64], Loss: 0.0005\n",
      "Epoch [3/10], Step [30/64], Loss: 0.0003\n",
      "Epoch [3/10], Step [40/64], Loss: 0.0003\n",
      "Epoch [3/10], Step [50/64], Loss: 0.0002\n",
      "Epoch [3/10], Step [60/64], Loss: 0.0003\n",
      "Validation - Epoch [3/10], SSIM: 0.8666, PSNR: 21.9513, MSE: 0.0057\n",
      "Epoch [4/10], Step [10/64], Loss: 0.0002\n",
      "Epoch [4/10], Step [20/64], Loss: 0.0003\n",
      "Epoch [4/10], Step [30/64], Loss: 0.0002\n",
      "Epoch [4/10], Step [40/64], Loss: 0.0002\n",
      "Epoch [4/10], Step [50/64], Loss: 0.0002\n",
      "Epoch [4/10], Step [60/64], Loss: 0.0002\n",
      "Validation - Epoch [4/10], SSIM: 0.9040, PSNR: 25.1557, MSE: 0.0027\n",
      "Epoch [5/10], Step [10/64], Loss: 0.0001\n",
      "Epoch [5/10], Step [20/64], Loss: 0.0001\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'UnidentifiedImageError' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m, in \u001b[0;36mBSDS500Dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert to RGB\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (UnidentifiedImageError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\PIL\\Image.py:937\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03mReturns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03mmethod translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m:returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    939\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      7\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m         inputs_upsampled \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(inputs, scale_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m, in \u001b[0;36mBSDS500Dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_name)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Convert to RGB\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[43mUnidentifiedImageError\u001b[49m, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Handle the error gracefully\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'UnidentifiedImageError' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs = data.to(device)\n",
    "        inputs_upsampled = F.interpolate(inputs, scale_factor=2, mode='bicubic', align_corners=False)\n",
    "        outputs = model(inputs_upsampled)\n",
    "        loss = criterion(outputs, inputs_upsampled)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    ssim_total, psnr_total, mse_total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs = data.to(device)\n",
    "            inputs_upsampled = F.interpolate(inputs, scale_factor=2, mode='bicubic', align_corners=False)\n",
    "            outputs = model(inputs_upsampled)\n",
    "            ssim_val, psnr_val, mse_val = calculate_metrics(outputs, inputs_upsampled)\n",
    "            ssim_total += ssim_val\n",
    "            psnr_total += psnr_val\n",
    "            mse_total += mse_val\n",
    "\n",
    "    print(f'Validation - Epoch [{epoch+1}/{num_epochs}], SSIM: {ssim_total/len(val_loader):.4f}, PSNR: {psnr_total/len(val_loader):.4f}, MSE: {mse_total/len(val_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a148262-7cf2-4488-9ffe-45849b5df8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    ssim_total, psnr_total, mse_total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs = data.to(device)\n",
    "            inputs_upsampled = F.interpolate(inputs, scale_factor=2, mode='bicubic', align_corners=False)\n",
    "            outputs = model(inputs_upsampled)\n",
    "            ssim_val, psnr_val, mse_val = calculate_metrics(outputs, inputs_upsampled)\n",
    "            ssim_total += ssim_val\n",
    "            psnr_total += psnr_val\n",
    "            mse_total += mse_val\n",
    "\n",
    "    print(f'Test - SSIM: {ssim_total/len(dataloader):.4f}, PSNR: {psnr_total/len(dataloader):.4f}, MSE: {mse_total/len(dataloader):.4f}')\n",
    "\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffbc294-fa4e-4731-9549-afb2e921465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function to visualize results on test set images\n",
    "def visualize_results(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            if data is None:  # Skip if data is None due to loading error\n",
    "                continue\n",
    "            \n",
    "            inputs = data.to(device)\n",
    "            inputs_upsampled = F.interpolate(inputs.clone(), scale_factor=2, mode='bicubic', align_corners=False)\n",
    "            outputs = model(inputs_upsampled)\n",
    "\n",
    "            # Display the input (original), upsampled image and output images from SRCNN.\n",
    "            fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "            axs[0].imshow(inputs[1].cpu().permute(1, 2, 0))\n",
    "            axs[0].set_title('Original Image')\n",
    "            axs[0].axis('off')  # Hide axes\n",
    "            \n",
    "            axs[1].imshow(inputs_upsampled[1].cpu().permute(1, 2, 0))\n",
    "            axs[1].set_title('Upsampled Image')\n",
    "            axs[1].axis('off')  \n",
    "            \n",
    "            axs[2].imshow(outputs[1].cpu().permute(1, 2, 0))\n",
    "            axs[2].set_title('SRCNN Output')\n",
    "            axs[2].axis('off')  \n",
    "            \n",
    "            plt.show()\n",
    "\n",
    "            # Only visualize one batch for simplicity; break after first batch.\n",
    "            break\n",
    "\n",
    "# Visualize results on the test set after evaluation.\n",
    "visualize_results(model,test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3f640-5520-4dc0-98da-1c1ae9ec7842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325a137-2fdc-4c4c-b445-8d51b642fec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49acb0d1-73c1-4d86-817a-183eae869136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ce7ce-55ab-4f09-9f58-9d5961b70192",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128))  # Set to a higher resolution\n",
    "])\n",
    "\n",
    "root_dir = \"BSR/BSDS500/data\"\n",
    "train_dataset = BSDS500Dataset(root_dir=root_dir, split='train', transform=transform)\n",
    "val_dataset = BSDS500Dataset(root_dir=root_dir, split='val', transform=transform)\n",
    "test_dataset = BSDS500Dataset(root_dir=root_dir, split='test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) #  load data in batches \n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0368e8f-f6bc-4f05-9449-5ffc8c0081bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module): # neural network module \n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__() # calls the constructor of the parent class (nn.Module) # it ensures that the nn.Module part of the SRCNN object is properly set up.\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "model = SRCNN().to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c9652-e817-4c1a-b89e-39dbdae5c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870ff0e0-e450-49dc-8efd-7c02ad1250e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(output, target):\n",
    "    output_np = output.permute(0, 2, 3, 1).cpu().numpy() #  Changes the order of dimensions of the tensor. If the original shape is (batch_size, channels, height, width), it changes it to (batch_size, height, width, channels).\n",
    "    target_np = target.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    ssim_val = np.mean([ssim(o, t, data_range=t.max() - t.min(), channel_axis=-1, win_size=5) for o, t in zip(output_np, target_np)])\n",
    "    psnr_val = np.mean([psnr(t, o, data_range=t.max() - t.min()) for o, t in zip(output_np, target_np)])\n",
    "    mse_val = np.mean((output_np - target_np) ** 2)\n",
    "    return ssim_val, psnr_val, mse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338c002-155e-4576-80e3-78c1e3d93ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def visualize_progress(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # No gradients: During evaluation or inference, we don't need to update the model, so calculating gradients is unnecessary and takes extra memory and computation.\n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs = data.to(device)\n",
    "            inputs_low_res = F.interpolate(inputs, scale_factor=0.5, mode='bicubic', align_corners=False)\n",
    "            outputs = model(inputs_low_res)\n",
    "            outputs_upsampled = F.interpolate(outputs, size=inputs.shape[2:], mode='bicubic', align_corners=False) # inputs (batch_size, channels, height, width) make the size of the target image as the input\n",
    "\n",
    "            # Display the input, low-res, and output images\n",
    "            fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            axs[0].imshow(inputs[0].cpu().permute(1, 2, 0))\n",
    "            axs[0].set_title('Original Image')\n",
    "            axs[1].imshow(inputs_low_res[0].cpu().permute(1, 2, 0))\n",
    "            axs[1].set_title('Low-Resolution Image')\n",
    "            axs[2].imshow(outputs_upsampled[0].cpu().permute(1, 2, 0))\n",
    "            axs[2].set_title('SRCNN Output')\n",
    "            plt.show()\n",
    "\n",
    "            # Only visualize one batch for simplicity\n",
    "            break\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs = data.to(device)\n",
    "        inputs_low_res = F.interpolate(inputs, scale_factor=0.5, mode='bicubic', align_corners=False)\n",
    "        outputs = model(inputs_low_res)\n",
    "        outputs_upsampled = F.interpolate(outputs, size=inputs.shape[2:], mode='bicubic', align_corners=False)\n",
    "        loss = criterion(outputs_upsampled, inputs)\n",
    "\n",
    "        optimizer.zero_grad() # resetting the gradients of all the model's parameters to zero. # Resetting: We need to reset (clear) the gradients to zero before computing the new gradients for the next batch.\n",
    "        loss.backward() # Calculates the gradients of the loss with respect to the model's parameters # understanding how much each parameter contributed to the error.\n",
    "        optimizer.step() # Updates the model's parameters using the gradients calculated \n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}') # 13 batches of data.\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    ssim_total, psnr_total, mse_total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs = data.to(device)\n",
    "            inputs_low_res = F.interpolate(inputs, scale_factor=0.5, mode='bicubic', align_corners=False)\n",
    "            outputs = model(inputs_low_res)\n",
    "            outputs_upsampled = F.interpolate(outputs, size=inputs.shape[2:], mode='bicubic', align_corners=False)\n",
    "            ssim_val, psnr_val, mse_val = calculate_metrics(outputs_upsampled, inputs)\n",
    "            ssim_total += ssim_val\n",
    "            psnr_total += psnr_val\n",
    "            mse_total += mse_val\n",
    "\n",
    "    print(f'Validation - Epoch [{epoch+1}/{num_epochs}], SSIM: {ssim_total/len(val_loader):.4f}, PSNR: {psnr_total/len(val_loader):.4f}, MSE: {mse_total/len(val_loader):.4f}')\n",
    "    \n",
    "    # Visualize progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        visualize_progress(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b770d-1c4d-4036-b7ac-43d4e0bc8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_one_random_image(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data_iter = iter(dataloader)\n",
    "        data = next(data_iter)\n",
    "        inputs = data.to(device)\n",
    "        inputs_low_res = F.interpolate(inputs, scale_factor=0.5, mode='bicubic', align_corners=False)\n",
    "        outputs = model(inputs_low_res)\n",
    "        outputs_upsampled = F.interpolate(outputs, size=inputs.shape[2:], mode='bicubic', align_corners=False)\n",
    "\n",
    "        # Calculate metrics\n",
    "        ssim_val, psnr_val, mse_val = calculate_metrics(outputs_upsampled, inputs)\n",
    "\n",
    "        # Display the input, low-res, and output images\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        axs[0].imshow(inputs[0].cpu().permute(1, 2, 0))\n",
    "        axs[0].set_title('Original Image')\n",
    "        axs[1].imshow(inputs_low_res[0].cpu().permute(1, 2, 0))\n",
    "        axs[1].set_title('Low-Resolution Image')\n",
    "        axs[2].imshow(outputs_upsampled[0].cpu().permute(1, 2, 0))\n",
    "        axs[2].set_title(f'SRCNN Output\\nPSNR: {psnr_val:.2f}, SSIM: {ssim_val:.4f}, MSE: {mse_val:.6f}')\n",
    "        plt.show()\n",
    "\n",
    "visualize_one_random_image(model, DataLoader(test_dataset, batch_size=1, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca78b21-b8af-4bdc-909c-5430426e638c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35cddccf-4fa0-44cb-bc09-105a229fa37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (1, 132, 132, 3)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'swin_ir_6/residual_swin_transformer_block_13/multi_head_attention_13/softmax_13/Softmax' defined at (most recent call last):\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n      super().run_forever()\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\IPS\\AppData\\Local\\Temp\\ipykernel_824\\1762359009.py\", line 83, in <module>\n      model.fit(train_images, target_images, epochs=10, batch_size=batch_size)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\AppData\\Local\\Temp\\ipykernel_824\\1762359009.py\", line 49, in call\n      x = self.rstb2(x)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\AppData\\Local\\Temp\\ipykernel_824\\1022104731.py\", line 18, in call\n      attn_output = self.attention(x, x)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 596, in call\n      attention_output, attention_scores = self._compute_attention(\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 527, in _compute_attention\n      attention_scores = self._masked_softmax(\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 493, in _masked_softmax\n      return self._softmax(attention_scores, attention_mask)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\layers\\activation\\softmax.py\", line 103, in call\n      return backend.softmax(inputs, axis=self.axis[0])\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\backend.py\", line 5413, in softmax\n      return tf.nn.softmax(x, axis=axis)\nNode: 'swin_ir_6/residual_swin_transformer_block_13/multi_head_attention_13/softmax_13/Softmax'\nOOM when allocating tensor with shape[4,4,4356,4356] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node swin_ir_6/residual_swin_transformer_block_13/multi_head_attention_13/softmax_13/Softmax}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_15107]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 83\u001b[0m\n\u001b[0;32m     80\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# Adjust this value based on your GPU memory limitations\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Train the model (example only)\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'swin_ir_6/residual_swin_transformer_block_13/multi_head_attention_13/softmax_13/Softmax' defined at (most recent call last):\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n      super().run_forever()\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\IPS\\AppData\\Local\\Temp\\ipykernel_824\\1762359009.py\", line 83, in <module>\n      model.fit(train_images, target_images, epochs=10, batch_size=batch_size)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\AppData\\Local\\Temp\\ipykernel_824\\1762359009.py\", line 49, in call\n      x = self.rstb2(x)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\AppData\\Local\\Temp\\ipykernel_824\\1022104731.py\", line 18, in call\n      attn_output = self.attention(x, x)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 596, in call\n      attention_output, attention_scores = self._compute_attention(\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 527, in _compute_attention\n      attention_scores = self._masked_softmax(\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 493, in _masked_softmax\n      return self._softmax(attention_scores, attention_mask)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\layers\\activation\\softmax.py\", line 103, in call\n      return backend.softmax(inputs, axis=self.axis[0])\n    File \"C:\\Users\\IPS\\.conda\\envs\\tf\\lib\\site-packages\\keras\\backend.py\", line 5413, in softmax\n      return tf.nn.softmax(x, axis=axis)\nNode: 'swin_ir_6/residual_swin_transformer_block_13/multi_head_attention_13/softmax_13/Softmax'\nOOM when allocating tensor with shape[4,4,4356,4356] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node swin_ir_6/residual_swin_transformer_block_13/multi_head_attention_13/softmax_13/Softmax}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_15107]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# Define a Residual Swin Transformer Block\n",
    "class ResidualSwinTransformerBlock(tf.keras.Model):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(ResidualSwinTransformerBlock, self).__init__()\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn = models.Sequential([\n",
    "            layers.Dense(embed_dim * 4, activation='relu'),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        attn_output = self.attention(x, x)\n",
    "        x = self.norm1(x + attn_output)  # Residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        return self.norm2(x + ffn_output)  # Residual connection\n",
    "\n",
    "# Define the SwinIR Model\n",
    "class SwinIR(tf.keras.Model):\n",
    "    def __init__(self, upscale_factor):\n",
    "        super(SwinIR, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(64, kernel_size=9, padding='same', activation='relu')\n",
    "        \n",
    "        # Residual Swin Transformer Blocks\n",
    "        self.rstb1 = ResidualSwinTransformerBlock(embed_dim=64, num_heads=4)\n",
    "        self.rstb2 = ResidualSwinTransformerBlock(embed_dim=64, num_heads=4)\n",
    "\n",
    "        # Final Convolution Layer\n",
    "        self.conv2 = layers.Conv2D(3 * (upscale_factor ** 2), kernel_size=5, padding='same')\n",
    "        \n",
    "        # Pixel Shuffle for upscaling\n",
    "        self.pixel_shuffle = layers.Lambda(lambda x: tf.nn.depth_to_space(x, upscale_factor))\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        # Reshape for attention blocks (batch_size, height*width, channels)\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        height = tf.shape(x)[1]\n",
    "        width = tf.shape(x)[2]\n",
    "        x = tf.reshape(x, (batch_size, height * width, 64))  # Reshape for attention blocks\n",
    "        \n",
    "        x = self.rstb1(x)\n",
    "        x = self.rstb2(x)\n",
    "\n",
    "        # Reshape back to spatial dimensions\n",
    "        x = tf.reshape(x, (batch_size, height, width, 64)) \n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        return self.pixel_shuffle(x)\n",
    "\n",
    "# Example usage of the model\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample input image (batch size of 1 and low resolution)\n",
    "    input_image = np.random.rand(1, 66, 66, 3).astype(np.float32)  # Low-resolution input\n",
    "\n",
    "    # Create the model\n",
    "    upscale_factor = 2\n",
    "    model = SwinIR(upscale_factor)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    output_image = model(tf.convert_to_tensor(input_image))\n",
    "\n",
    "    print(\"Output shape:\", output_image.shape)  # Should be (1, 132, 132, 3)\n",
    "\n",
    "    # Compile the model for training (example only)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Example training data (dummy data for demonstration)\n",
    "    train_images = np.random.rand(100, 66, 66, 3).astype(np.float32)  # Low-resolution images\n",
    "    target_images = np.random.rand(100, 132, 132, 3).astype(np.float32)  # High-resolution images\n",
    "\n",
    "    # Reduce batch size to address potential OOM error\n",
    "    batch_size = 4  # Adjust this value based on your GPU memory limitations\n",
    "\n",
    "    # Train the model (example only)\n",
    "    model.fit(train_images, target_images, epochs=10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaadeae8-28a6-43c8-8209-6bc3a93c1e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
